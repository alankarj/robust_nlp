This dataset consists of 5 million paraphrases, generated by backtranslation of the Czeng1.6 corpus. For details see "Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations". These paraphrases can be used to produce state-of-the-art paraphrastic sentence embeddings as well as be used for downstream tasks and paraphrase generation.

The format of the dataset is <reference sentence> <translation> <paragram-phrase score>. This tokenized dataset consists of filtered paraphrases, by selecting the middle percentiles of the paraphram-phrase score and translation length (<= 30 tokens). Filtering by paragram-phrase score and keeping hte middle percentiles is recommeneded to remove noisy and trivial paraphrases. It is also recommened to keep only those paraphrases where the translations aren't long, a maximum of 30 tokens seems to work well.

The full dataset of 50M+ paraphrases is also available on http://www.cs.cmu.edu/~jwieting.

If you use either of these datasets for your work please cite:

@inproceedings{wieting-17-millions,
        author = {John Wieting and Kevin Gimpel},
        title = {Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations},
        booktitle = {arXiv preprint arXiv:1711.05732},
        year = {2017}
}

@inproceedings{wieting-17-backtrans,
        author = {John Wieting, Jonathan Mallinson, and Kevin Gimpel},
        title = {Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext},
        booktitle = {Proceedings of Empirical Methods in Natural Language Processing},
        year = {2017}
}

